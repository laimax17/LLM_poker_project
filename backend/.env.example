# ── Ollama 本地推理 ───────────────────────────────────────────────────────────
# Ollama 服务地址。宿主机直接运行时用 localhost；Docker 内访问宿主机用 host.docker.internal
OLLAMA_HOST=http://localhost:11434

# Ollama 使用的模型名，需与 `ollama pull <model>` 时一致
# 推荐：qwen2.5:7b（中文强，速度快）；备选：qwen2.5:14b、llama3.1:8b
OLLAMA_MODEL=qwen2.5:7b

# ── 阿里云 DashScope（Qwen 云端）────────────────────────────────────────────
# 在 https://dashscope.console.aliyun.com 创建，严禁提交到 Git
DASHSCOPE_API_KEY=your_dashscope_api_key_here

# DashScope 使用的模型：qwen-turbo（最快）/ qwen-plus（推荐）/ qwen-max（最强）
QWEN_MODEL=qwen-plus

# ── 游戏引擎 ──────────────────────────────────────────────────────────────────
# 服务启动时使用的默认 AI 引擎
# 可选值：rule-based | gto | ollama | qwen-plus | qwen-max
# 完整配置说明见 docs/llm-playbook.md
DEFAULT_AI_ENGINE=rule-based
